{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on azure batch 12.0.0 and azure storage blob 12.9.0\n"
     ]
    }
   ],
   "source": [
    "import azure.batch\n",
    "import azure.storage.blob\n",
    "print(f\"working on azure batch {azure.batch.__version__} and azure storage blob {azure.storage.blob.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch._batch_service_client as batch\n",
    "import azure.batch.batch_auth as batch_auth\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from azbatch import main\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on batch account 510adagpu\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "config = {\n",
    "    \"POOL_ID\": \"ADA_pool_NC6\", # f\"job_{start_time.strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"JOB_ID\":  f\"job_{start_time.strftime('%Y%m%d%H%M')}\",\n",
    "    \"POOL_NODE_COUNT\": 52,  # max 312 cores of NCpromo --> 26 x NC12 or 52 x NC6\n",
    "    \"POOL_VM_SIZE\": \"Standard_NC6_Promo\",\n",
    "    \"TASK_SLOTS_PER_NODE\": 1,  # keep <= cores per machine\n",
    "\n",
    "    \"BATCH_ACCOUNT_NAME\": os.environ.get(\"_BATCH_ACCOUNT_NAME\"),\n",
    "    \"BATCH_ACCOUNT_KEY\": os.environ.get(\"_BATCH_ACCOUNT_KEY\"),\n",
    "    \"BATCH_ACCOUNT_URL\": os.environ.get(\"_BATCH_ACCOUNT_URL\"),\n",
    "\n",
    "    \"CR_PASSWORD\": os.environ.get(\"_CR_PASSWORD\"),  # container registry\n",
    "    \n",
    "    \"ADA_STORAGE_ACCOUNT_NAME\": os.environ.get(\"_ADA_STORAGE_ACCOUNT_NAME\"),\n",
    "    \"ADA_STORAGE_ACCOUNT_KEY\": os.environ.get(\"_ADA_STORAGE_ACCOUNT_KEY\"),\n",
    "    \"510_STORAGE_ACCOUNT_NAME\": os.environ.get(\"_510_STORAGE_ACCOUNT_NAME\"),\n",
    "    \"510_STORAGE_ACCOUNT_KEY\": os.environ.get(\"_510_STORAGE_ACCOUNT_KEY\"),\n",
    "    \n",
    "    \"510_DLS_CONNECTION_STRING\": os.environ.get(\"_510_DLS_CONNECTION_STRING\"),\n",
    "    \"XCCTEST_CONNECTION_STRING\": os.environ.get(\"_XCCTEST_CONNECTION_STRING\")\n",
    "}\n",
    "\n",
    "print(f'working on batch account {config[\"BATCH_ACCOUNT_NAME\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to batch & storage accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch\n",
    "# service in addition to Storage\n",
    "batch_client = batch.BatchServiceClient(\n",
    "    credentials=batch_auth.SharedKeyCredentials(\n",
    "        account_name=config[\"BATCH_ACCOUNT_NAME\"], \n",
    "        key=config[\"BATCH_ACCOUNT_KEY\"],\n",
    "    ),\n",
    "    batch_url=config[\"BATCH_ACCOUNT_URL\"]\n",
    ")\n",
    "\n",
    "blob_client_xcctest = azureblob.BlobServiceClient.from_connection_string(config[\"XCCTEST_CONNECTION_STRING\"])\n",
    "blob_client_510 = azureblob.BlobServiceClient.from_connection_string(config[\"510_DLS_CONNECTION_STRING\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pool & job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [ADA_pool_NC6]...\n",
      "Created pool ADA_pool_NC6.\n"
     ]
    }
   ],
   "source": [
    "# Create the pool that will contain the compute nodes that will execute the\n",
    "# tasks.\n",
    "if not batch_client.pool.exists(config['POOL_ID']):\n",
    "    pool = main.create_pool(batch_client, config)\n",
    "    print(f\"Created pool {config['POOL_ID']}.\")\n",
    "else:\n",
    "    print(f\"Pool {config['POOL_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [job_202203221521]...\n",
      "Created job job_202203221521.\n"
     ]
    }
   ],
   "source": [
    "# Create the job that will run the tasks.\n",
    "if not config['JOB_ID'] in [j.id for j in batch_client.job.list()]:\n",
    "    main.create_job(batch_client, config)\n",
    "    print(f\"Created job {config['JOB_ID']}.\")\n",
    "else:\n",
    "    print(f\"Job {config['JOB_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container & storage settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common settings \n",
    "task_container_settings = batchmodels.TaskContainerSettings(\n",
    "    image_name='ada510.azurecr.io/ada:latest',\n",
    "    # ipc=host needed for pytorch to share memory \n",
    "    # https://discuss.pytorch.org/t/unable-to-write-to-file-torch-18692-1954506624/9990\n",
    "    container_run_options='--rm --ipc=host'\n",
    ")\n",
    "# needed to create folders inside running container\n",
    "admin_identity = batchmodels.UserIdentity(\n",
    "    auto_user=batchmodels.AutoUserSpecification(\n",
    "        scope='pool',\n",
    "        elevation_level='admin',\n",
    "    )\n",
    ")\n",
    "task_common_args = {\n",
    "    \"container_settings\": task_container_settings,\n",
    "    \"user_identity\": admin_identity,\n",
    "}\n",
    "\n",
    "upload_opts = batchmodels.OutputFileUploadOptions(\n",
    "    upload_condition=batchmodels.OutputFileUploadCondition.task_success\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonly used tokens & urls\n",
    "adafiles_read_token = main.create_sas_token(config[\"ADA_STORAGE_ACCOUNT_NAME\"], config[\"ADA_STORAGE_ACCOUNT_KEY\"], \"adafiles\", [\"read\", \"list\"])\n",
    "adafiles_write_token = main.create_sas_token(config[\"ADA_STORAGE_ACCOUNT_NAME\"], config[\"ADA_STORAGE_ACCOUNT_KEY\"], \"adafiles\", [\"write\"])\n",
    "_510_read_token = main.create_sas_token(config[\"510_STORAGE_ACCOUNT_NAME\"], config[\"510_STORAGE_ACCOUNT_KEY\"], \"automated-damage-assessment\", [\"read\", \"list\"])\n",
    "adafiles_output_url = main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_write_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data directory and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"tropical-storm-ana\"  # relative to container\n",
    "date = \"2022-01-22\" # YYYY-MM-DD\n",
    "tile_index_filename = \"tile_index_maxar.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images and create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_name = datetime.datetime.now().strftime('%Y%m%d%H%M%S')  # necessary to match dependencies\n",
    "tasks = []\n",
    "\n",
    "tasks += [\n",
    "    # # download images ==> TO BE DONE MANUALLY\n",
    "    # batchmodels.TaskAddParameter(\n",
    "    #     id=f\"download-images\",\n",
    "    #     depends_on=None,\n",
    "    #     command_line=f'/bin/bash -c \"load-images '\\\n",
    "    #                  f'--disaster {data_dir} '\\\n",
    "    #                  f'--maxthreads 4 '\\\n",
    "    #                  f'--dest {data_dir}\"',\n",
    "    #     output_files=[\n",
    "    #         batchmodels.OutputFile(\n",
    "    #             file_pattern=f\"{data_dir}/**\",\n",
    "    #             destination=batchmodels.OutputFileDestination(\n",
    "    #                 container=batchmodels.OutputFileBlobContainerDestination(\n",
    "    #                     container_url=adafiles_output_url,\n",
    "    #                     path=f\"{data_dir}\",\n",
    "    #                 )\n",
    "    #             ),\n",
    "    #             upload_options=upload_opts,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     **task_common_args,\n",
    "    # ),\n",
    "    \n",
    "#     # create index\n",
    "#     batchmodels.TaskAddParameter(\n",
    "#         id=f\"create-index\",\n",
    "#         depends_on=None,\n",
    "#         command_line=f'/bin/bash -c \"create-index '\\\n",
    "#                      f'--data {data_dir} '\\\n",
    "#                      f'--date {date} '\\\n",
    "#                      f'--zoom 13 '\\\n",
    "#                      f'--dest tile_index.geojson'\\\n",
    "#                      f'--exte extents\"',\n",
    "#         resource_files=[\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "#                 blob_prefix=f\"{data_dir}\"\n",
    "#             )\n",
    "#         ],\n",
    "#         output_files=[\n",
    "#             batchmodels.OutputFile(\n",
    "#                 file_pattern=f\"tile_index.geojson\",\n",
    "#                 destination=batchmodels.OutputFileDestination(\n",
    "#                     container=batchmodels.OutputFileBlobContainerDestination(\n",
    "#                         container_url=adafiles_output_url,\n",
    "#                         path=f\"{data_dir}/tile_index.geojson\",\n",
    "#                     )\n",
    "#                 ),\n",
    "#                 upload_options=upload_opts,\n",
    "#             )\n",
    "#         ],\n",
    "#         **task_common_args,\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Add tasks to job\n",
    "# res = batch_client.task.add_collection(config['JOB_ID'], tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index length: 75\n"
     ]
    }
   ],
   "source": [
    "index_client = blob_client_xcctest.get_blob_client(container=\"adafiles\", blob=f\"{data_dir}/{tile_index_filename}\")\n",
    "try:\n",
    "    with open(tile_index_filename, \"wb\") as download_file:\n",
    "        download_file.write(index_client.download_blob().readall())\n",
    "except:\n",
    "   print(\"No blob found.\")\n",
    "\n",
    "with open(tile_index_filename) as file:\n",
    "    index = json.load(file)\n",
    "    \n",
    "# remove duplicates\n",
    "features = index['features'].copy()\n",
    "index.pop('features', None)\n",
    "index['features'] = []\n",
    "for feature in features:\n",
    "    if not any([feature['properties']['tile'] == x['properties']['tile'] for x in index['features']]):\n",
    "        index['features'].append(feature)\n",
    "    \n",
    "print(f\"index length: {len(index['features'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ids 75\n"
     ]
    }
   ],
   "source": [
    "batch_name = datetime.datetime.now().strftime('%Y%m%d%H%M%S')  # necessary to match dependencies\n",
    "unique_ids = [tile['properties']['tile'] for tile in index['features']]\n",
    "tasks = []\n",
    "print(f\"unique ids {len(unique_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building detection tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tasks_abd(batch_name, num_id, id_, dependencies_start, data=\"raw/pre-event/merged.tif\", dest=\"pre-event\"):\n",
    "    \n",
    "    tasks = [\n",
    "    \n",
    "        # abd cover: create cover file with metadata of mini-tiles\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"cover-{batch_name}-{num_id}-{dest}\",\n",
    "            depends_on=dependencies_start,\n",
    "            command_line=f'/bin/bash -c \"abd cover --raster merged.tif --zoom 17 --out cover.csv\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/{data}\"),\n",
    "                    file_path='merged.tif'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"cover.csv\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/{dest}/cover.csv\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd tile: split tiles in mini-tiles\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"tile-{batch_name}-{num_id}-{dest}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"cover-{batch_name}-{num_id}-{dest}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd tile --raster merged.tif --zoom 17 --cover cover.csv --config config.toml --out images --format tif --keep_borders\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/{data}\"),\n",
    "                    file_path='merged.tif'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/{dest}/cover.csv\"),\n",
    "                    file_path='cover.csv'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"images/**/*.tif\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/{dest}/images\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd predict: predict buildings on mini-tiles -- only runnable on a GPU instance !!!\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"predict-{batch_name}-{num_id}-{dest}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"tile-{batch_name}-{num_id}-{dest}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd predict --config config.toml --cover cover.csv --dataset {id_}/{dest} '\\\n",
    "                         f'--checkpoint neat-fullxview-epoch75.pth --out predictions --metatiles --keep_borders\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"neat-fullxview-epoch75.pth\"),\n",
    "                    file_path='neat-fullxview-epoch75.pth'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/{dest}/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/{dest}/cover.csv\"),\n",
    "                    file_path='cover.csv'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"predictions/**/*.png\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/{dest}/predictions\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd vectorize: convert pixel-level predictions into polygons (.geojson)\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"vectorize-{batch_name}-{num_id}-{dest}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"predict-{batch_name}-{num_id}-{dest}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd vectorize --config config.toml --masks {id_}/{dest}/predictions --out buildings.geojson --type Building\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/{dest}/predictions/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/{dest}/buildings.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),    \n",
    "\n",
    "        # filter buildings\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"filter-buildings-{batch_name}-{num_id}-{dest}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"vectorize-{batch_name}-{num_id}-{dest}\"]),\n",
    "            command_line=f'/bin/bash -c \"filter-buildings --data buildings.geojson --dest buildings-clean.geojson --waterbodies hydropolys.gpkg\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/{dest}/buildings.geojson\"),\n",
    "                    file_path='buildings.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"hydropolys.gpkg\"),\n",
    "                    file_path='hydropolys.gpkg'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings-clean.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/{dest}/buildings-clean.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "    ]\n",
    "    return tasks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare workdir, building detection, filter and align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add tasks separately for each tile\n",
    "for num_id, id_ in enumerate(unique_ids): #  39, 40, 41\n",
    "    \n",
    "    num_id = num_id#+start_\n",
    "    \n",
    "    images_to_process = list(index['features'][num_id]['properties']['pre-event'].values()) + list(index['features'][num_id]['properties']['post-event'].values())\n",
    "    images_to_process_resource_files = []\n",
    "    for image in images_to_process:\n",
    "        images_to_process_resource_files.append(batchmodels.ResourceFile(\n",
    "            http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{data_dir}/{image}\"),\n",
    "            file_path=f'{data_dir}/{image}'\n",
    "        ))\n",
    "    \n",
    "    tasks += [\n",
    "        \n",
    "        # set up working directory and create raster mosaic (--> merged.tif)\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"setup-{batch_name}-{num_id}\",\n",
    "            depends_on=None,\n",
    "            command_line=f'/bin/bash -c \"setup-wd --data {data_dir} --index tile_index.geojson --id {id_} --dest raw --maxar-tiling\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{data_dir}/{tile_index_filename}\"),\n",
    "                    file_path='tile_index.geojson'\n",
    "                )\n",
    "            ] + images_to_process_resource_files,\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"raw/**/*.tif\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"temp/{id_}/raw\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # adding prefix to id_ path\n",
    "    id_ = f\"temp/{id_}\"\n",
    "    \n",
    "    # detect buildings in pre-disaster raster\n",
    "    tasks += tasks_abd(batch_name,\n",
    "                       num_id,\n",
    "                       id_,\n",
    "                       dependencies_start=batchmodels.TaskDependencies(task_ids=[f\"setup-{batch_name}-{num_id}\"]),\n",
    "                       data=\"raw/pre-event/merged.tif\",\n",
    "                       dest=\"pre-event\")\n",
    "    \n",
    "    # check if buildings already exist, if yes substitute\n",
    "    tasks += [\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"check-alternative-buildings-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"filter-buildings-{batch_name}-{num_id}-pre-event\"]),\n",
    "            command_line=f'/bin/bash -c \"check-alternative-buildings '\\\n",
    "                         f'--ext extents.geojson '\\\n",
    "                         f'--builds google-africa-buildings-split '\\\n",
    "                         f'--container adafiles '\\\n",
    "                         f'--raster pre-merged.tif '\\\n",
    "                         f'--refbuilds pre-buildings.geojson '\\\n",
    "                         f'--dest new-pre-buildings.geojson ' \\\n",
    "                         f'--secret {config[\"XCCTEST_CONNECTION_STRING\"]}\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"google-africa-buildings-split/extents.geojson\"),\n",
    "                    file_path='extents.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/pre-event/buildings-clean.geojson\"),\n",
    "                    file_path='pre-buildings.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/raw/pre-event/merged.tif\"),\n",
    "                    file_path='pre-merged.tif'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"new-pre-buildings.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/pre-event/buildings-clean.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # detect buildings in post-disaster raster\n",
    "    tasks += tasks_abd(batch_name,\n",
    "                       num_id, id_,\n",
    "                       dependencies_start=batchmodels.TaskDependencies(task_ids=[f\"check-alternative-buildings-{batch_name}-{num_id}\"]),\n",
    "                       data=\"raw/post-event/merged.tif\",\n",
    "                       dest=\"post-event\")\n",
    "    \n",
    "    tasks += [\n",
    "        # align post-disaster raster\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"align-raster-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"filter-buildings-{batch_name}-{num_id}-post-event\"]),\n",
    "            command_line=f'/bin/bash -c \"align-raster '\\\n",
    "                         f'--targetbuild post-buildings.geojson '\\\n",
    "                         f'--referencebuild pre-buildings.geojson '\\\n",
    "                         f'--alignedbuild post-buildings-aligned.geojson '\\\n",
    "                         f'--targetraster post-merged.tif '\\\n",
    "                         f'--alignedraster post-merged.tif\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/post-event/buildings-clean.geojson\"),\n",
    "                    file_path='post-buildings.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/pre-event/buildings-clean.geojson\"),\n",
    "                    file_path='pre-buildings.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/raw/post-event/merged.tif\"),\n",
    "                    file_path='post-merged.tif'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"post-buildings-aligned.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/post-event/buildings-clean-aligned.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                ),\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"post-merged-aligned.tif\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/raw/post-event/merged.tif\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                ),\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### damage classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"attentive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num_id, id_ in enumerate(unique_ids):\n",
    "    \n",
    "    num_id = num_id#+start_\n",
    "    # adding prefix to id_ path\n",
    "    id_ = f\"temp/{id_}\"\n",
    "    \n",
    "    tasks += [\n",
    "        \n",
    "        # prepare for caladrius\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"prepare-data-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"align-raster-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"prepare-data --data {id_}/raw --datapre merged --datapost merged '\\\n",
    "                         f'--buildings buildings-clean.geojson --dest caladrius\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/raw/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/pre-event/buildings-clean.geojson\"),\n",
    "                    file_path='buildings-clean.geojson'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"caladrius/**/*.png\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/caladrius\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "        \n",
    "        # run caladrius\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"run-caladrius-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"prepare-data-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"source ~/.bashrc && source activate cal && '\\\n",
    "                         f'CUDA_VISIBLE_DEVICES=\"0\" python /caladrius/caladrius/run.py --run-name run --data-path {id_}/caladrius '\\\n",
    "                         f'--model-type {model_type} '\\\n",
    "                         f'--model-path model_weights.pkl '\\\n",
    "                         f'--batch-size 2 '\\\n",
    "                         f'--classification-loss-type f1 '\\\n",
    "                         f'--checkpoint-path caladrius --output-type classification --inference\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/caladrius\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"caladrius_att_effnet4_v1.pkl\"),\n",
    "                    file_path=f'model_weights.pkl'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"caladrius/**/*\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/caladrius\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "        \n",
    "        # merge buildings and damage labels\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"final-layer-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"run-caladrius-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"final-layer --builds buildings-clean.geojson --damage damage-labels.txt --out buildings-predictions.geojson\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/pre-event/buildings-clean.geojson\"),\n",
    "                    file_path='buildings-clean.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \n",
    "                                                      container_path=f\"{id_}/caladrius/run-input_size_32-learning_rate_0.001-batch_size_2/predictions/run-split_inference-epoch_001-model_{model_type}-predictions.txt\"),\n",
    "                    file_path='damage-labels.txt'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings-predictions.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/buildings-predictions.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tasks to job\n",
    "res = batch_client.task.add_collection(config['JOB_ID'], tasks)\n",
    "# res.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all outputs\n",
    "### N.B. wait for tasks to be finished!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all outputs\n",
    "task = [\n",
    "    batchmodels.TaskAddParameter(\n",
    "        id=f\"merge-all-outputs\",\n",
    "        depends_on=None,\n",
    "        command_line=f'/bin/bash -c \"merge-output '\\\n",
    "                     f'--dir temp '\\\n",
    "                     f'--dest buildings-predictions.geojson\"',\n",
    "        resource_files=[\n",
    "            batchmodels.ResourceFile(\n",
    "                storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                blob_prefix=f\"temp\"\n",
    "            )\n",
    "        ],\n",
    "        output_files=[\n",
    "            batchmodels.OutputFile(\n",
    "                file_pattern=\"buildings-predictions.geojson\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{data_dir}/buildings-predictions.geojson\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )\n",
    "        ],\n",
    "        **task_common_args,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Add tasks to job\n",
    "res = batch_client.task.add_collection(config['JOB_ID'], task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Batch resources\n",
    "### N.B. wait for tasks to be finished!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current job\n",
    "batch_client.job.delete(config['JOB_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current pool\n",
    "batch_client.pool.delete(config['POOL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all jobs\n",
    "for job in batch_client.job.list():\n",
    "    batch_client.job.delete(job.id)\n",
    "\n",
    "# delete all pools\n",
    "for pool in batch_client.pool.list():\n",
    "    batch_client.pool.delete(pool.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "azure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batch_auth\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "from azbatch import main\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "config = {\n",
    "    \"POOL_ID\": \"pool_20200921103445\", # f\"job_{start_time.strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"JOB_ID\":  f\"job_{start_time.strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"POOL_NODE_COUNT\": 1,\n",
    "    \"POOL_VM_SIZE\": \"STANDARD_D2_V2\",  # or \"Standard_NC6\"\n",
    "\n",
    "    \"BATCH_ACCOUNT_NAME\": os.environ.get(\"_BATCH_ACCOUNT_NAME\"),\n",
    "    \"BATCH_ACCOUNT_KEY\": os.environ.get(\"_BATCH_ACCOUNT_KEY\"),\n",
    "    \"BATCH_ACCOUNT_URL\": os.environ.get(\"_BATCH_ACCOUNT_URL\"),\n",
    "\n",
    "    \"CR_PASSWORD\": os.environ.get(\"_CR_PASSWORD\"),  # container registry\n",
    "    \n",
    "    \"STORAGE_ACCOUNT_NAME\": os.environ.get(\"_STORAGE_ACCOUNT_NAME\"),\n",
    "    \"STORAGE_ACCOUNT_KEY\": os.environ.get(\"_STORAGE_ACCOUNT_KEY\"),\n",
    "    \n",
    "    \"510_DLS_CONNECTION_STRING\": os.environ.get(\"_510_DLS_CONNECTION_STRING\"),\n",
    "    \"XCCTEST_CONNECTION_STRING\": os.environ.get(\"_XCCTEST_CONNECTION_STRING\"),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to batch & storage accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch\n",
    "# service in addition to Storage\n",
    "batch_client = batch.BatchServiceClient(\n",
    "    credentials=batch_auth.SharedKeyCredentials(\n",
    "        account_name=config[\"BATCH_ACCOUNT_NAME\"], \n",
    "        key=config[\"BATCH_ACCOUNT_KEY\"],\n",
    "    ),\n",
    "    batch_url=config[\"BATCH_ACCOUNT_URL\"]\n",
    ")\n",
    "\n",
    "blob_client_xcctest = azureblob.BlockBlobService(connection_string=config[\"XCCTEST_CONNECTION_STRING\"])\n",
    "blob_client_510 = azureblob.BlockBlobService(connection_string=config[\"510_DLS_CONNECTION_STRING\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pool & job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pool that will contain the compute nodes that will execute the\n",
    "# tasks.\n",
    "if not batch_client.pool.exists(config['POOL_ID']):\n",
    "    main.create_pool(batch_client, config)\n",
    "    print(f\"Created pool {config['POOL_ID']}.\")\n",
    "else:\n",
    "    print(f\"Pool {config['POOL_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the job that will run the tasks.\n",
    "if not config['JOB_ID'] in [j.id for j in batch_client.job.list()]:\n",
    "    main.create_job(batch_client, config)\n",
    "    print(f\"Created job {config['JOB_ID']}.\")\n",
    "else:\n",
    "    print(f\"Job {config['JOB_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicating the following steps from the pipeline:\n",
    "```\n",
    "neo cover --raster ~/datalake/maxar/typhoon-mangkhut/processed/pre-event/*-ntl.tif --zoom 17 --out ~/datalake/maxar/typhoon-mangkhut/neo/cover.csv\n",
    "\n",
    "neo tile --raster ~/datalake/maxar/typhoon-mangkhut/processed/pre-event/*-ntl.tif --zoom 17 --cover ~/datalake/maxar/typhoon-mangkhut/neo/cover.csv --config ~/neateo/config.toml --out ~/datalake/maxar/typhoon-mangkhut/neo/images --format tif\n",
    "\n",
    "mkdir ~/datalake/maxar/typhoon-mangkhut/neo/predictions\n",
    "\n",
    "neo predict --config ~/neateo/config.toml --dataset ~/datalake/maxar/typhoon-mangkhut/neo --cover ~/datalake/maxar/typhoon-mangkhut/neo/cover.csv --checkpoint ~/datalake/neateo-models/neat-fullxview-epoch75.pth --out ~/datalake/maxar/typhoon-mangkhut/neo/predictions --metatiles --keep_borders\n",
    "\n",
    "neo vectorize --masks ~/datalake/maxar/typhoon-mangkhut/neo/predictions --type Building --config ~/neateo/config.toml --out ~/datalake/maxar/typhoon-mangkhut/processed/buildings.geojson\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container & storage settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common settings \n",
    "task_container_settings = batchmodels.TaskContainerSettings(\n",
    "    image_name=main.NEO_IMAGE,\n",
    "    container_run_options='--rm'  # maybe necessary to use `--gpus all`?\n",
    ")\n",
    "admin_identity = batchmodels.UserIdentity(\n",
    "    auto_user=batchmodels.AutoUserSpecification(\n",
    "        scope='pool',\n",
    "        elevation_level='admin',\n",
    "    )\n",
    ")\n",
    "task_common_args = {\n",
    "    \"container_settings\": task_container_settings,\n",
    "    \"user_identity\": admin_identity,\n",
    "}\n",
    "\n",
    "upload_opts = batchmodels.OutputFileUploadOptions(\n",
    "    upload_condition=batchmodels.OutputFileUploadCondition.task_success\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonly used tokens & urls\n",
    "adafiles_read_token = main.create_sas_token(blob_client_xcctest, \"adafiles\", [\"read\", \"list\"])\n",
    "adafiles_write_token = main.create_sas_token(blob_client_xcctest, \"adafiles\", [\"write\"])\n",
    "_510_read_token = main.create_sas_token(blob_client_510, \"automated-damage-assessment\", [\"read\", \"list\"])\n",
    "\n",
    "adafiles_output_url = main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_write_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual task specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = datetime.datetime.now().strftime('%Y%m%d%H%M%S')  # necessary to match dependencies\n",
    "\n",
    "tasks = [    \n",
    "    # neo cover\n",
    "    batchmodels.TaskAddParameter(\n",
    "        id=f\"cover-{batch_name}\",\n",
    "        depends_on=None,\n",
    "        command_line='/bin/bash -c \"wd=$AZ_BATCH_TASK_WORKING_DIR && neo cover --raster $wd/processed.tif --zoom 17 --out $wd/cover.csv\"',\n",
    "        resource_files=[\n",
    "            batchmodels.ResourceFile(\n",
    "                http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=\"taskout/ada/pre-event/103001007E413300-3013212.tif\"),\n",
    "                file_path='processed.tif'\n",
    "            )\n",
    "        ],\n",
    "        output_files=[\n",
    "            batchmodels.OutputFile(\n",
    "                file_pattern=\"cover.csv\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=\"cover.csv\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )\n",
    "        ],\n",
    "        **task_common_args,\n",
    "    ),\n",
    "\n",
    "    # neo tile\n",
    "    batchmodels.TaskAddParameter(\n",
    "        id=f\"tile-{batch_name}\",\n",
    "#         depends_on=batchmodels.TaskDependencies(task_ids=[f\"cover-{batch_name}\"]),\n",
    "        command_line='/bin/bash -c \"wd=$AZ_BATCH_TASK_WORKING_DIR && neo tile --raster $wd/processed.tif --zoom 17 --cover $wd/cover.csv --config $wd/config.toml --out $wd/images --format tif\"',\n",
    "        resource_files=[\n",
    "            batchmodels.ResourceFile(\n",
    "                http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=\"taskout/ada/pre-event/103001007E413300-3013212.tif\"),\n",
    "                file_path='processed.tif'\n",
    "            ),\n",
    "            batchmodels.ResourceFile(\n",
    "                http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                file_path='config.toml'\n",
    "            ),\n",
    "            batchmodels.ResourceFile(\n",
    "                http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"cover.csv\"),\n",
    "                file_path='cover.csv'\n",
    "            ),\n",
    "        ],\n",
    "        output_files=[\n",
    "            batchmodels.OutputFile(\n",
    "                file_pattern=\"images/**/*.tif\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=\"images\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )\n",
    "        ],\n",
    "        **task_common_args,\n",
    "    ),\n",
    "\n",
    "#     # neo predict -- !!! only runnable on a GPU instance\n",
    "#     batchmodels.TaskAddParameter(\n",
    "#         id=f\"predict-{batch_name}\",\n",
    "#         depends_on=batchmodels.TaskDependencies(task_ids=[f\"tile-{batch_name}\"]),\n",
    "#         command_line='/bin/bash -c \"wd=$AZ_BATCH_TASK_WORKING_DIR && neo predict --config $wd/config.toml --cover $wd/cover.csv --dataset XXXX --checkpoint $wd/neat-fullxview-epoch75.pth --out $wd/predictions  --metatiles --keep_borders\"',\n",
    "#         resource_files=[\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=\"taskout/ada/pre-event/103001007E413300-3013212.tif\"),\n",
    "#                 file_path='processed.tif'\n",
    "#             ),\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 http_url=main.create_resource_url(\"510datalakestorage\", \"automated-damage-assessment\", _510_read_token, container_path=\"neateo-models/neat-fullxview-epoch75.pth\"),\n",
    "#                 file_path='neat-fullxview-epoch75.pth'\n",
    "#             ),\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "#                 blob_prefix='images/',\n",
    "#                 file_path='images/',\n",
    "#             ),\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "#                 file_path='config.toml'\n",
    "#             ),\n",
    "#             batchmodels.ResourceFile(\n",
    "#                 http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"cover.csv\"),\n",
    "#                 file_path='cover.csv'\n",
    "#             ),\n",
    "#         ],\n",
    "#         output_files=[batchmodels.OutputFile(\n",
    "#             file_pattern=\"predictions/*\",\n",
    "#             destination=batchmodels.OutputFileDestination(\n",
    "#                 container=batchmodels.OutputFileBlobContainerDestination(\n",
    "#                     container_url=adafiles_output_url,\n",
    "#                     path=\"predictions\",\n",
    "#                 )\n",
    "#             ),\n",
    "#             upload_options=upload_opts,\n",
    "#         )],\n",
    "#         **task_common_args,\n",
    "#     ),\n",
    "\n",
    "    # neo vectorize\n",
    "    batchmodels.TaskAddParameter(\n",
    "        id=f\"vectorize-{batch_name}\",\n",
    "        depends_on=None,  # batchmodels.TaskDependencies(task_ids=[f\"predict-{batch_name}\"]),\n",
    "        command_line='/bin/bash -c \"wd=$AZ_BATCH_TASK_WORKING_DIR && neo vectorize --config $wd/config.toml --masks $wd/testpredict/predictions --out $wd/buildings.geojson --type Building\"',\n",
    "        resource_files=[\n",
    "            batchmodels.ResourceFile(\n",
    "#                 storage_container_url=main.create_resource_url(\"510datalakestorage\", \"automated-damage-assessment\", _510_read_token),\n",
    "#                 blob_prefix=\"maxar/typhoon-mangkhut-2/neo/predictions/\",\n",
    "                storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                blob_prefix=\"testpredict/predictions/\",\n",
    "            ),\n",
    "            batchmodels.ResourceFile(\n",
    "                http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                file_path='config.toml'\n",
    "            ),\n",
    "        ],\n",
    "        output_files=[\n",
    "            batchmodels.OutputFile(\n",
    "                file_pattern=\"buildings.geojson\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=\"buildings.geojson\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )\n",
    "        ],\n",
    "        **task_common_args,\n",
    "    ),    \n",
    "]\n",
    "\n",
    "# Add tasks to job\n",
    "res = batch_client.task.add_collection(config['JOB_ID'], tasks)\n",
    "res.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Batch resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current job / pool\n",
    "batch_client.job.delete(config['JOB_ID'])\n",
    "batch_client.pool.delete(config['POOL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all jobs\n",
    "for job in batch_client.job.list():\n",
    "    batch_client.job.delete(job.id)\n",
    "\n",
    "# delete all pools\n",
    "for pool in batch_client.pool.list():\n",
    "    batch_client.pool.delete(pool.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

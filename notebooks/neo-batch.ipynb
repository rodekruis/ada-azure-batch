{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch._batch_service_client as batch\n",
    "import azure.batch.batch_auth as batch_auth\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from azbatch import main\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.0.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azure.batch\n",
    "azure.batch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on batch account 510adagpu\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "config = {\n",
    "    \"POOL_ID\": \"test_pool_NC12\", # f\"job_{start_time.strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"JOB_ID\":  f\"job_{start_time.strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"POOL_NODE_COUNT\": 12,  # max 312 cores of NCpromo --> 26 x NC12 or 52 x NC6\n",
    "    \"POOL_VM_SIZE\": \"Standard_NC12_Promo\",\n",
    "    \"TASK_SLOTS_PER_NODE\": 1,  # keep <= cores per machine\n",
    "\n",
    "    \"BATCH_ACCOUNT_NAME\": os.environ.get(\"_BATCH_ACCOUNT_NAME\"),\n",
    "    \"BATCH_ACCOUNT_KEY\": os.environ.get(\"_BATCH_ACCOUNT_KEY\"),\n",
    "    \"BATCH_ACCOUNT_URL\": os.environ.get(\"_BATCH_ACCOUNT_URL\"),\n",
    "\n",
    "    \"CR_PASSWORD\": os.environ.get(\"_CR_PASSWORD\"),  # container registry\n",
    "    \n",
    "    \"STORAGE_ACCOUNT_NAME\": os.environ.get(\"_STORAGE_ACCOUNT_NAME\"),\n",
    "    \"STORAGE_ACCOUNT_KEY\": os.environ.get(\"_STORAGE_ACCOUNT_KEY\"),\n",
    "    \n",
    "    \"510_DLS_CONNECTION_STRING\": os.environ.get(\"_510_DLS_CONNECTION_STRING\"),\n",
    "    \"XCCTEST_CONNECTION_STRING\": os.environ.get(\"_XCCTEST_CONNECTION_STRING\")\n",
    "}\n",
    "\n",
    "print(f'working on batch account {config[\"BATCH_ACCOUNT_NAME\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to batch & storage accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch\n",
    "# service in addition to Storage\n",
    "batch_client = batch.BatchServiceClient(\n",
    "    credentials=batch_auth.SharedKeyCredentials(\n",
    "        account_name=config[\"BATCH_ACCOUNT_NAME\"], \n",
    "        key=config[\"BATCH_ACCOUNT_KEY\"],\n",
    "    ),\n",
    "    batch_url=config[\"BATCH_ACCOUNT_URL\"]\n",
    ")\n",
    "\n",
    "blob_client_xcctest = azureblob.BlockBlobService(connection_string=config[\"XCCTEST_CONNECTION_STRING\"])\n",
    "blob_client_510 = azureblob.BlockBlobService(connection_string=config[\"510_DLS_CONNECTION_STRING\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pool & job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [test_pool_NC12]...\n",
      "Created pool test_pool_NC12.\n"
     ]
    }
   ],
   "source": [
    "# Create the pool that will contain the compute nodes that will execute the\n",
    "# tasks.\n",
    "if not batch_client.pool.exists(config['POOL_ID']):\n",
    "    pool = main.create_pool(batch_client, config)\n",
    "    print(f\"Created pool {config['POOL_ID']}.\")\n",
    "else:\n",
    "    print(f\"Pool {config['POOL_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [job_20210714145356]...\n",
      "Created job job_20210714145356.\n"
     ]
    }
   ],
   "source": [
    "# Create the job that will run the tasks.\n",
    "if not config['JOB_ID'] in [j.id for j in batch_client.job.list()]:\n",
    "    main.create_job(batch_client, config)\n",
    "    print(f\"Created job {config['JOB_ID']}.\")\n",
    "else:\n",
    "    print(f\"Job {config['JOB_ID']} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container & storage settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common settings \n",
    "task_container_settings = batchmodels.TaskContainerSettings(\n",
    "    image_name='ada510.azurecr.io/ada:latest',\n",
    "    # ipc=host needed for pytorch to share memory \n",
    "    # https://discuss.pytorch.org/t/unable-to-write-to-file-torch-18692-1954506624/9990\n",
    "    container_run_options='--rm --ipc=host'\n",
    ")\n",
    "# needed to create folders inside running container\n",
    "admin_identity = batchmodels.UserIdentity(\n",
    "    auto_user=batchmodels.AutoUserSpecification(\n",
    "        scope='pool',\n",
    "        elevation_level='admin',\n",
    "    )\n",
    ")\n",
    "task_common_args = {\n",
    "    \"container_settings\": task_container_settings,\n",
    "    \"user_identity\": admin_identity,\n",
    "}\n",
    "\n",
    "upload_opts = batchmodels.OutputFileUploadOptions(\n",
    "    upload_condition=batchmodels.OutputFileUploadCondition.task_success\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonly used tokens & urls\n",
    "adafiles_read_token = main.create_sas_token(blob_client_xcctest, \"adafiles\", [\"read\", \"list\"])\n",
    "adafiles_write_token = main.create_sas_token(blob_client_xcctest, \"adafiles\", [\"write\"])\n",
    "_510_read_token = main.create_sas_token(blob_client_510, \"automated-damage-assessment\", [\"read\", \"list\"])\n",
    "adafiles_output_url = main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_write_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data directory and read index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index length: 129\n",
      "{'type': 'Feature', 'properties': {'tile': '12.1150.1730', 'pre-event': {'0': 'pre-event/1030010086A20400.tif', '1': 'pre-event/1030010095164900.tif'}, 'post-event': {'0': 'post-event/1050010018A3AC00.tif', '1': 'post-event/1050010018A3AD00.tif'}}, 'geometry': {'type': 'Polygon', 'coordinates': [[[-78.837890625, 26.902476886279814], [-78.837890625, 26.82407078047018], [-78.92578125, 26.82407078047018], [-78.92578125, 26.902476886279814], [-78.837890625, 26.902476886279814]]]}}\n",
      "{'type': 'Feature', 'properties': {'tile': '12.1150.1731', 'pre-event': {'0': 'pre-event/1030010086A20400.tif', '1': 'pre-event/1030010095164900.tif'}, 'post-event': {'0': 'post-event/1050010018A3AC00.tif', '1': 'post-event/1050010018A3AD00.tif'}}, 'geometry': {'type': 'Polygon', 'coordinates': [[[-78.837890625, 26.82407078047018], [-78.837890625, 26.745610382199015], [-78.92578125, 26.745610382199015], [-78.92578125, 26.82407078047018], [-78.837890625, 26.82407078047018]]]}}\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"hurricane-dorian\"  # relative to container\n",
    "blob_client_xcctest.get_blob_to_path(container_name=\"adafiles\",\n",
    "                                     blob_name=data_dir+'/tile_index.geojson',\n",
    "                                     file_path='tile_index.geojson')\n",
    "with open('tile_index.geojson') as file:\n",
    "    index = json.load(file)\n",
    "print(f\"index length: {len(index['features'])}\")\n",
    "for ind_ in index['features'][0:2]:\n",
    "    print(ind_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual task specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0, 12.1150.1730\n",
      "['pre-event/1030010086A20400.tif', 'pre-event/1030010095164900.tif', 'post-event/1050010018A3AC00.tif', 'post-event/1050010018A3AD00.tif']\n",
      "TASK 1, 12.1150.1731\n",
      "['pre-event/1030010086A20400.tif', 'pre-event/1030010095164900.tif', 'post-event/1050010018A3AC00.tif', 'post-event/1050010018A3AD00.tif']\n"
     ]
    }
   ],
   "source": [
    "batch_name = datetime.datetime.now().strftime('%Y%m%d%H%M%S')  # necessary to match dependencies\n",
    "unique_ids = [tile['properties']['tile'] for tile in index['features']]\n",
    "tasks = []\n",
    "\n",
    "# add tasks separately for each tile\n",
    "for num_id, id_ in enumerate(unique_ids):\n",
    "    \n",
    "    num_id = num_id#+start_\n",
    "    \n",
    "    images_to_process = list(index['features'][num_id]['properties']['pre-event'].values()) + list(index['features'][num_id]['properties']['post-event'].values())\n",
    "    images_to_process_resource_files = []\n",
    "    for image in images_to_process:\n",
    "        images_to_process_resource_files.append(batchmodels.ResourceFile(\n",
    "            http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{data_dir}/{image}\"),\n",
    "            file_path=f'{data_dir}/{image}'\n",
    "        ))\n",
    "        \n",
    "    print(f'TASK {num_id}, {id_}')\n",
    "    print(images_to_process)\n",
    "    \n",
    "    tasks += [\n",
    "        \n",
    "        # set up working directory and create raster mosaic (--> merged.tif)\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"setup-{batch_name}-{num_id}\",\n",
    "            depends_on=None,\n",
    "            command_line=f'/bin/bash -c \"setup-wd --data {data_dir} --index tile_index.geojson --id {id_} --dest raw\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{data_dir}/tile_index.geojson\"),\n",
    "                    file_path='tile_index.geojson'\n",
    "                )\n",
    "            ] + images_to_process_resource_files,\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"raw/**/*.tif\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/raw\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd cover: create cover file with metadata of mini-tiles\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"cover-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"setup-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd cover --raster merged.tif --zoom 17 --out cover.csv\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/raw/pre-event/merged.tif\"),\n",
    "                    file_path='merged.tif'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"cover.csv\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/cover.csv\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd tile: split tiles in mini-tiles\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"tile-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"cover-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd tile --raster merged.tif --zoom 17 --cover cover.csv --config config.toml --out images --format tif\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/raw/pre-event/merged.tif\"),\n",
    "                    file_path='merged.tif'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/cover.csv\"),\n",
    "                    file_path='cover.csv'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"images/**/*.tif\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/images\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd predict: predict buildings on mini-tiles -- only runnable on a GPU instance !!!\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"predict-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"tile-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd predict --config config.toml --cover cover.csv --dataset {id_} --checkpoint neat-fullxview-epoch75.pth --out predictions --metatiles --keep_borders\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"neat-fullxview-epoch75.pth\"),\n",
    "                    file_path='neat-fullxview-epoch75.pth'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/cover.csv\"),\n",
    "                    file_path='cover.csv'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"predictions/**/*.png\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/predictions\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "\n",
    "        # abd vectorize: convert pixel-level predictions into polygons (.geojson)\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"vectorize-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"predict-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"abd vectorize --config config.toml --masks {id_}/predictions --out buildings.geojson --type Building\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/predictions/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"config.toml\"),\n",
    "                    file_path='config.toml'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/buildings.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),    \n",
    "        \n",
    "        # filter buildings\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"filter-buildings-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"vectorize-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"filter-buildings --data buildings.geojson --dest buildings-clean.geojson --waterbodies hydropolys.gpkg\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/buildings.geojson\"),\n",
    "                    file_path='buildings.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"hydropolys.gpkg\"),\n",
    "                    file_path='hydropolys.gpkg'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings-clean.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/buildings-clean.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "        \n",
    "        # prepare for caladrius\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"prepare-data-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"filter-buildings-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"prepare-data --data {id_}/raw --buildings buildings-clean.geojson --dest caladrius\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/raw/\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/buildings-clean.geojson\"),\n",
    "                    file_path='buildings-clean.geojson'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"caladrius/**/*.png\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/caladrius\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "        \n",
    "        # run caladrius\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"run-caladrius-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"prepare-data-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"source ~/.bashrc && source activate cal && '\\\n",
    "                         f'python /caladrius/caladrius/run.py --run-name run --data-path {id_}/caladrius '\\\n",
    "                         f'--model-path best_model_wts.pkl '\\\n",
    "                         f'--checkpoint-path caladrius --output-type classification --inference\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    storage_container_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token),\n",
    "                    blob_prefix=f\"{id_}/caladrius\"\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, \"best_model_wts.pkl\"),\n",
    "                    file_path=f'best_model_wts.pkl'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[batchmodels.OutputFile(\n",
    "                file_pattern=\"caladrius/**/*\",\n",
    "                destination=batchmodels.OutputFileDestination(\n",
    "                    container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                        container_url=adafiles_output_url,\n",
    "                        path=f\"{id_}/caladrius\",\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=upload_opts,\n",
    "            )],\n",
    "            **task_common_args,\n",
    "        ),\n",
    "        \n",
    "        # merge buildings and damage labels\n",
    "        batchmodels.TaskAddParameter(\n",
    "            id=f\"final-layer-{batch_name}-{num_id}\",\n",
    "            depends_on=batchmodels.TaskDependencies(task_ids=[f\"run-caladrius-{batch_name}-{num_id}\"]),\n",
    "            command_line=f'/bin/bash -c \"final-layer --builds buildings-clean.geojson --damage damage-labels.txt --out buildings-predictions.geojson\"',\n",
    "            resource_files=[\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/buildings-clean.geojson\"),\n",
    "                    file_path='buildings-clean.geojson'\n",
    "                ),\n",
    "                batchmodels.ResourceFile(\n",
    "                    http_url=main.create_resource_url(\"xcctest\", \"adafiles\", adafiles_read_token, container_path=f\"{id_}/caladrius/run-input_size_32-learning_rate_0.001-batch_size_32/predictions/run-split_inference-epoch_001-model_inception-predictions.txt\"),\n",
    "                    file_path='damage-labels.txt'\n",
    "                )\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    file_pattern=\"buildings-predictions.geojson\",\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            container_url=adafiles_output_url,\n",
    "                            path=f\"{id_}/buildings-predictions.geojson\",\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=upload_opts,\n",
    "                )\n",
    "            ],\n",
    "            **task_common_args,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Add tasks to job\n",
    "res = batch_client.task.add_collection(config['JOB_ID'], tasks)\n",
    "# res.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Batch resources\n",
    "### N.B. wait for tasks to be finished!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current job\n",
    "batch_client.job.delete(config['JOB_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current pool\n",
    "batch_client.pool.delete(config['POOL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all jobs\n",
    "for job in batch_client.job.list():\n",
    "    batch_client.job.delete(job.id)\n",
    "\n",
    "# delete all pools\n",
    "for pool in batch_client.pool.list():\n",
    "    batch_client.pool.delete(pool.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:azure]",
   "language": "python",
   "name": "conda-env-azure-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
